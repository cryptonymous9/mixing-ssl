dino:
  # Add DINO-specific parameters here

# Common configurations
model:
  arch: vit_base
  remove_head: 0
  mlp: "2048-512"
  mlp_coeff: 1.0
  patch_keep: 1.0
  fc: 0
  proj_relu: 0

training:
  eval_only: 0
  eval_freq: 100
  batch_size: 512
  num_crops: 1
  optimizer: adamw  # Options: ['sgd', 'adamw', 'lars']
  momentum: 0.9
  weight_decay: 4e-5
  epochs: 30
  base_lr: 0.0005
  end_lr_ratio: 0.001
  label_smoothing: 0.0
  distributed: 0
  clip_grad: 0.0
  use_ssl: 1
  loss: dino
  train_probes_only: 0
  mixup: 0.0

data:
  train_dataset: ""  # Required, override with your dataset
  val_dataset: ""
  num_workers: 10
  in_memory: true  # Required, specify if dataset fits in memory

validation:
  batch_size: 25
  resolution: 224

logging:
  folder: ""  # Required, specify log folder
  log_level: 2
  checkpoint_freq: 20